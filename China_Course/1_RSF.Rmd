---
title: "Introduction to Resource Selection Function Models"
author: "Jared Stabach, Smithsonian National Zoo & Conservation Biology Institute"
date: '`r format(Sys.time(), "%d %B %Y")`'
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: true
      smooth_scroll: true
    number_sections: false
    #theme: united
    #highlight: tango
pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<a href="https://github.com/Smithsonian/Wildebeest_RSF.git" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

# Introduction
Habitat selection is the process by which animals use certain habitat types/resources. If an animal is located in a habitat type more than would be expected by random, then it is assumed that the animal is selecting for the habitat.  In contrast, if an animal uses a particular habitat/resource less than it's availability, it is assumed that the animal avoids or does not prefer the habitat/resource.  Determining habitat use patterns is critical to the effective management and conservation of animals.  Global positioning system data allow fine-scale assessments of habitat selection.  These data are typically analyzed in a resource selection function (RSF) or step selection function (SSF) framework.  In each of these cases, animal *'use'* locations are contrasted with random locations (the *'availability'* sample).  Differences, however, exist between these two frameworks, which importantly include how much or how little (or at all) the movement process is included.  RSF models are point-process models where the movement process is largely ignored, with many arguing that a step selection function more effectively incorporates the movement process.  I believe that both frameworks can be used effectively, dependent on your research question and scale of inference.  

In this script, we'll introduce the basic approach to Resource Selection Function (RSF) modeling. We'll integrate the wildebeest dataset that we've been using in the past few exercises to conduct a point-based analyses to evaluate habitat suitability.  This vignette is meant as a first-step towards more complex models which incorporate the animal movement process.  We will be relying primarily on the functionality provided for RSF analysis by the package [amt](https://cran.r-project.org/web/packages/amt/index.html). We'll address the following objectives here:

  * Some habitats are preferred over other habitats
  * **Resource Selection Functions** quantify preference and avoidance
  * The general strategy is to compare *'used'* locations (GPS data, presence locations) with locations that are *'available'* (pseudo-absence)
    + Incorporate raster layers that are thought *a-priori* to affect space use
    + Generate a random sample of availability
    + Extract raster values at each 'use' and 'available' location
    + Perform logistic regression 
    $$ log[p_i/(1-p_i)] = logit(p_i) = \beta_0 + \beta_1x_1 + \beta_2x_2 + ...+ \beta_nx_n $$
    and
    $$ p_i = \frac{exp(\beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + ... + \beta_nx_n)}{1+exp(\beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + ... + \beta_nx_n)}$$
    But, for use/availablity designs, we drop the intercept (its meaningless), and focus only on:
    $$ w(x,\beta) = exp(\beta_1x_1 + \beta_2x_2 + ...+ \beta_nx_n)$$
    + Positive $\beta$'s indicate preference; Negative $\beta$'s indicate avoidance
    + Generate a map, providing a prediction of habitat use
    
We'll be following details provided in the following publications:

Fieberg, J., J. Signer, B. Smith, and T. Avgar. 2021. A ‘How to’ guide for interpreting parameters in habitat‐selection analyses. [Journal of Animal Ecology 00:1-17](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/1365-2656.13441).

Signer, J., J. Fieberg, and T. Avgar. 2019. Animal movement tools (amt): R package for managing tracking data and conducting habitat selection analyses. [Ecology and evolution 9(2): 880-890](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6362447/).

Stabach, J.A., G. Wittemyer, R.B. Boone, R.S. Reid, J.S. Worden. 2016. Variation in habitat selection by white-bearded wildebeest across different degrees of human disturbance. [Ecosphere 7(8)](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecs2.1428).

## Setup
Load libraries and setup your workspace for analyses.

```{r RSF Libraries, message=FALSE, warning=FALSE}
# Remove everything from memory
rm(list=ls())

# Set.seed - This function means that any generation of "random" points/numbers will be the same each time you run the script.
set.seed(533)

# You may need to install these packages first
#install.packages('amt', 'tidyverse', 'lme4', 'terra', 'sf', 'sjPlot', 'usdm', 'visreg', 'tmap', 'AICcmodavg')

# Load libraries
library(amt)
library(tidyverse)
library(lme4)
library(terra)
library(sf)
library(sjPlot) # provide plot_model()
#library(usdm)
library(visreg) # provides vis_reg() for plotting response curves
library(tmap)
#library(AICcmodavg) # provides aic_tab() 

# Set all tmaps to plot in view mode
tmap_mode("view")
```
  
## Load Data
Read in all your spatial data.  The animal movement data is a 'flat' dataframe (a non-projected file) with coordinates that have been extracted in UTM 37 S, WGS84 (previous exercise).  The raster layers are projected to Albers Equal Area.  We will need to project the point data to match the raster data for analyses.

Note, the projection used for analyses doesn't matter too much, as long as you are consistent between the data layers (point and raster) you are using.  It is important, however, to use a projection that minimizes the amount of distortion across your study area.

```{r RSF Load}
# Load a polygon shapefile of the study area.  File is project to Albers Equal Area.  This will be our prediction area.
Athi.Bound <- st_read("Data/Athi.shp")

# Load all raster layers for Athi-Kaputiei Plains study area.  The raster stack consists of 7 raster layers.  To be in a stack they all have to have the exact same resolution (250-m) and spatial extent. 
rsf.stack <- terra::rast("data/ak_raster_stack.tif")
plot(rsf.stack)

# Data Layers included:
# anth_risk - Anthropogenic Risk, simply an index of human footprint made specifically for this ecosystem (expected negative response)
# Fence_dist - Fence Distance, with fences manually mapped by a Kenyan field team (expected negative response)
# prirds_dist - Primary Road Distance, the distance from primary/paved roads (expected negative response)
# secrds_dist - Secondary Road Distance, the distance from secondary/unpaved roads (expected null response)
# river_dist - River Distance, the distance from permanent rivers (depending on season, but wildebeest must drink - greater attraction in dry season, but a predation risk)
# waterpts_dist - Water Point Distance, the distance to mapped water wells (expected positive response, stronger in dry season)
# woody_dist -Woody Distance, the distance to woody vegetation (based on VCF) (expected negative response - predation risk)

# The fence plot is hard to interpret because there are so many low values in the center of the plot. This means there are a lot of fences there. Let's color the distance values and only show the first 100 to better see where the fences are located.
# plot(rsf.stack$fence_dist)
plot(rsf.stack$fence_dist,
            range = c(0, 100),
            col = "black")

# We can review the range of values in each raster like this.
# summary(values(rsf.stack$fence_dist))

# What is the crs of the raster stack?
crs(rsf.stack, proj=TRUE)

# *******************************
# Now let's import the GPS tracking dataset.  
# This is the 3-hour, regular trajectory.
# Let's import and remove the trajectory information
# We also need to remove the na values in the x/y coordinates
WB.data <- read_rds("Data/wildebeest_3hr_adehabitat.rds") %>% 
  select(x,
         y,
         date,
         id,
         sex) %>% 
  filter(!is.na(x),
         !is.na(y),
         !is.na(date))

# Is this a spatial object?
#class(WB.data)
# Is it projected?
#st_crs(WB.data)
# What is the timezone?
#tz(WB.data$date)

# Here's where we need to be careful.
# Our raster layers are projected to Albers Equal Area.  Our point layer is not projected, but the x/y coordinates are derived from when the layer was projected to UTM 37S, WGS84.
# This means we should sent the file to UTM37S, WGS84 and project to Albers Equal Area
WB.data.sf <- WB.data %>%  
  st_as_sf(coords = c('x', 'y'),
           crs = "EPSG:32737") %>% # This is UTM 37S, WGS84 (UtmZone.proj <- "EPSG:32737")
  st_transform(crs(rsf.stack)) # Easiest way to set the projection is grab it from the crs of our already defined raster stack

# The specific project is Albers Equal Area, specific to Africa.  This would be defined as:
#AEA.Africa.proj <- "+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"

# Now what's the CRS?
# st_crs(WB.data.sf)

# Let's plot to make sure everything is aligned.
# Here I'm choosing to plot with a sequential palette with 10 colors on the ramp. You can run tmaptools::palette_explorer() to explore the palette options. 
tm_shape(rsf.stack$anth_risk,
         name = "Human Risk Level") +
  tm_raster(palette = "YlOrRd", n = 10,
            alpha = 0.6,
            title = "Anth. Risk") +
  tm_shape(WB.data.sf %>% 
             filter(id == "Noontare"),
           name = "Noontare Locations") +
  tm_dots(size = 0.01,
          col = "black") + 
  tm_shape(Athi.Bound,
           name = "Athi-Kaputiei Plains") +
  tm_polygons(alpha = 0,
              border.col = "green") + 
  tm_layout("Example Graph")

# This first raster represents anthropogenic risk. And you can see that this wildebeest appears to be responding to this variable. If you zoom into the northern locations, as there are no locations in the areas of elevated human risk.

# Let's plot one more. Let's look at this wildebeest's points with distance to primary road. Here I want the low values to be red, instead of the high values, so I'm reversing the palette with the "-". I'm also including a lot more colors, otherwise it was difficult to see where the roads actually were. This makes the legend too large though so I'm hiding the legend for the raster.

# tm_shape(rsf.stack$prirds_dist,
#          name = "Distance to Primary Rd") +
#   tm_raster(palette = "-YlOrRd", n = 30,
#             alpha = 0.6,
#             legend.show = FALSE) +
#   tm_shape(WB.data.sf %>%
#              filter(id == "Noontare"),
#            name = "Noontare Locations") +
#   tm_dots(size = 0.01,
#           col = "gray")

# Check tmap_options()
# This is where the default basemaps are set.  This is why we see ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap
```

# Fitting a Model for a Single Animal
Here will take the initial steps to fit a RSF to a single individual using the [amt](https://cran.r-project.org/web/packages/amt/index.html) package.

## Prepare Data for Modeling
[Amt](https://cran.r-project.org/web/packages/amt/index.html) relies heavily on piping, which has a bit of a (steep) learning curve, but can help in streamlining the data processing workflow. We'll start by creating a trajectory (called a track in [amt](https://cran.r-project.org/web/packages/amt/index.html)).

Remember also that RSF analyses assume statistical independence between locations and habitat covariates. We could provide a very conservative estimate of time to independence using [ctmm](https://cran.r-project.org/web/packages/ctmm/index.html) and then resample our data to some interval greater than this time scale.  This will surely reduce our sample size.  As an example, we will thin the data heavily, sampling 20% of the data randomly for analyses.

```{r Create Subset, message=FALSE, warning=FALSE, echo=TRUE}
# Create Subset of dataset, filtering to 1 animal
nt <- WB.data.sf %>% 
  mutate(x = st_coordinates(WB.data.sf)[ ,1],
         y = st_coordinates(WB.data.sf)[ ,2]) %>% 
  as_tibble() %>% 
  select(-geometry) %>%  
  filter(id == "Ntishya") %>% 
  mutate(id = droplevels(id))
  
# Create Movement Track (similar to ltraj function in adehabitatLT)
# We don't really need the time component for this initial analysis.  That would be a step-selection function (ssf) analysis.  But, we'll include here as an example.
nt.trk <- mk_track(nt, 
                   .x = x, 
                   .y = y,
                   .t = date, 
                   crs = crs(WB.data.sf), # Alternative just include crs = 9822.  9822 is the EPSG code for AEA.
                   order_by_ts = T,
                   id = id, 
                   sex = sex)

# Note that amt does not calculate steplengths or turning angles when you make the track.  To do so, you'd need to specify those variables:
# nt.trk.test <- nt.trk %>% 
#   mutate(
#     sl = step_lengths(.),
#     ta = direction_abs(.) # absolute and relative turning angles can be calculated (direction_rel())
#     )

# Remember also that the track should be kept regular, otherwise we have potential to bias our results (e.g., more points in the day than at night). Our track has already been resampled to a 3 hour interval in a previous lecture
# In AMT, we would do this by using the track_resample command:
nt.trk.rs <- track_resample(nt.trk,
                            rate = hours(3), 
                            tolerance = minutes(20))

# Let's thin the data as an example   
rcd.amt <- ceiling(nrow(nt.trk.rs)*0.20) # ceiling just rounds up to a whole number
nt.trk.rs <- nt.trk.rs[sample(nrow(nt.trk.rs),size = rcd.amt),c("id","x_","y_","id","sex")]
```
  
## Defining Availability
Perhaps the most important aspect of any habitat selection analysis is the assessment of availability.  In a RSF, the assumption is that the distribution of available habitat is constant through time and that the animal has equal access to **ALL** areas within a user-defined area (Type III Habitat Selection).  This assumption may hold true when positions are observed infrequently (the case more commonly when data were collected infrequently from VHF studies), but less so with many modern telemetry studies.

The goal is to contrast points where an animal was observed with locations that were potentially available.  To do so, we will create pseudo-absence points generated throughout an animals' home range.  The [amt](https://cran.r-project.org/web/packages/amt/index.html) function `random_points` is a convenient way to do this, with various options for generating the area in which sample.

```{r Availability, message=FALSE, warning=FALSE, echo=TRUE}
# Here we will generate 10 times the number of "Use" points to get our list of "available" points. We'll start with this level to begin to inspect our data and look for initial patterns. Before running our final models we'll assess what a robust minimum # of available points should be.

# Create simple boundary of points
hr <- hr_mcp(nt.trk.rs, levels = 1) # Amt has multiple options, including hr_akde().  Levels indicates the isopleth (Here: 100%).  We could also upload our own polygon.

# Generate random points within the defined polygon.  Important to include the type (random or regular) that we want to be generated.
nt.rsf.10 <- random_points(hr,
                           n = nrow(nt.trk.rs) * 10,
                           type ="regular",
                           presence = nt.trk.rs) 

# What does our dataset look like now?
head(nt.rsf.10)
# Cool, the function created a 'case_' field, tracking our 'Use' points (TRUE) and our 'Availability' points (FALSE)

# Summarize
table(nt.rsf.10$case_)
#class(nt.rsf.10)

# Plot Use/Availability
plot(nt.rsf.10) 
```

## Extract Covariates
Now that our 'Use'/'Availability' dataset has been constructed, we will extract the value of each raster at these points.   We will use the function `extract_covariates()` which is simply a wrapper function in [amt](https://cran.r-project.org/web/packages/amt/index.html) that uses the `terra::extract()` function you are now familiar with.

```{r Extract, message=FALSE, warning=FALSE, echo=TRUE}
# Extract all the raster variables at Use/Available points.
nt.rsf.10 <- nt.rsf.10 %>% 
  extract_covariates(rsf.stack)

# Look at result
head(nt.rsf.10)
#names(nt.rsf.10)

# Now let's summarize these results to get a sense of how the values compare between 'use' and 'available' locations
nt.rsf.10 %>% 
  ggplot(aes(y = anth_risk,
             col = case_)) + 
  geom_boxplot() +
  labs(title = "Anthropogenic Risk")

# You may have many covariates, so doing this in a loop makes sense. Instead using a 'for' loop, we'll use the 'map' function.  See 'help(map)'.  First we need to make a vector of variable names that we want to plot.
vars <- nt.rsf.10 %>% 
  select(anth_risk:woody_dist) %>% 
  names()

# The, we simply use 'map' to create a plot for each var.  the '.x' is like our i in our 'for' loop.
box.plots <- map(vars, ~
                   nt.rsf.10 %>% 
                   select(case_, var = .x) %>% 
                   ggplot(aes(y = var,
                              col = case_)) + 
                   geom_boxplot() + 
                   labs(title = .x))

# We can put them all together using the cowplot
cowplot::plot_grid(plotlist = box.plots)
```

## Assess Collinearity
Collinear predictor (indepedent) variables can influence the variance estimates of the model and make it difficult to interpret model coefficients.  This isn't a problem if prediction is your goal.  However, in our case, we are interested in understanding the influence of these predictors on our response variable (i.e., where our animals are located in space and time).  As a result, we need to avoid multi-collinearity. 

We can get a general sense of pairwise problems between predictor variables by looking at correlation values.  As a general rule of thumb, values > 0.7 are recognized as being problematic. We can also check the Variance Inflation Factor (VIF) which assess multi-collinearity across ALL predictors, with higher values indicating high collinearity of a predictor with the rest of the set.  Some sources indicate a VIF over 10 is problematic, while others set the threshold at 3.0. 

```{r Collinearity, message=FALSE, warning=FALSE, echo=TRUE}
# Assess collinearity
cor(nt.rsf.10[,4:10])

# The table of correlation values indicates that high levels of correlation exist between fence distance and waterpoint distance (0.84).  We should remove one of these variables based on our research objectives.  Here, I'll keep fence distance because I am more interested in this question from a management standpoint.  Some other correlations are observed between woody distance and waterpoints and woody distance and primary roads.

# Before making any decisions, let's also check the Variance inflation Factor. 
vif(as.data.frame(nt.rsf.10[,4:10]))

# Let's remove waterpoints and see if doing so is helpful
vif(as.data.frame(nt.rsf.10[,c(4:8,10)]))

# This helps a alot, but woody distance is still quite high (VIF  = 5.7).  We could make a decision to remove entirely or if we think it could be an important factor (e.g., woody vegetation is likely avoided by wildebeest because of increased risk of predation), we could include it while making sure woody vegetation was not included in the same model with fences and primary roads.  We could then evaluate each model separately using AIC.  For now, we'll proceed.
```

## Sensitivity Analyses - How many Locations are Enough?
Before we move on to final model fitting, we must first address whether we have adequately sampled 'Availability'.  Ultimately, there is a trade-off between statistical robustness and computer processing time.  I generally perform this on a single individual (as we are doing here) as evidence that I am appropriately sampling from availability.  

Our goal is to determine the optimal sample of availability necessary to achieve stable coefficient estimates.  To do so, we must iterate (or "loop") over different sets of availability, from 1 'Availability' point per 'Use' point to 100 'Availability' points per 'Use' point.  We will use a nested structure, rather than the "for loop" structure that we have demonstrated in previous exercises.  For this somewhat complicated process, I think the nested structure is a bit easier.  We then do this process multiple times, so that the 'Available' points differ and so we can evaluate how much the coefficients change between repetitive model fittings.

```{r Sensitivity, message=FALSE, warning=FALSE, echo=TRUE}
# Setup number of available locations to sample during each model fitting
n.frac <- c(1, 5, 20, 50, 100) 

# Total available locations to be generated, based on the number of Use points
n.pts <- nrow(nt.trk.rs) * n.frac

# Number of repetitions.  This is simulation so that we can evaluate the variability that exists in the coefficients
# For a publication, I would increase the n.rep to 100.  Here, 20 is fine.
n.rep <- 20

# Create a table which saves the settings of each scenario
# We then extract the covariates during each repetition from rsf.stack during each run (the points will vary)
# Then, fit a glm model for each available location (1,5,20,50,100) and for each replication (re-sampling)

# Run simulation and store results
# Commented out as the process is time consuming
# **********************************************
# wb.sim <- tibble(
#  n.pts = rep(n.pts, n.rep),
#  frac = rep(n.frac, n.rep),
#  result = map(
#     n.pts, ~
#       nt.trk.rs %>% random_points(n = .x) %>%
#       extract_covariates(rsf.stack) %>%
#       mutate(anth_risk = scale(anth_risk),
#              woody_dist = scale(woody_dist),
#              fence_dist = scale(fence_dist),
#              prirds_dist = scale(prirds_dist),
#              river_dist = scale(river_dist),
#              secrds_dist = scale(secrds_dist),
#              waterpts_dist = scale(waterpts_dist)) %>%
#       # Fit basic model
#       glm(case_ ~ anth_risk +
#             woody_dist +
#             fence_dist +
#             prirds_dist +
#             river_dist +
#             secrds_dist +
#             waterpts_dist,
#           data = ., family = binomial(link = "logit")) %>%
#       broom::tidy()))

# Save file so don't need to run everytime
# write_rds(wb.sim, file = "Data/nt.sim.rds")

# Read in the result
wb.sim <- read_rds("Data/nt.sim.rds")

# Look at the summary table of results
# This shows that we have 100 rows, summarizing the number of points (n.pts) and the availability fraction (frac)
# We have 100 rows because we have 5 different fractions (n.rep = 5 -> 1,5,20,50,100) and we ran the simulations (n.rep = 20) times.  5 x 20 = 100 rows of results
wb.sim 
# Look at the first simulation result
wb.sim$result[[1]]

# Visualize/Graph findings
# We must 'unnest' the contents in each nested dataframe so visualize the coefficient estimates from individual model fits
wb.sim %>% unnest(cols = result) %>% 
  mutate(term = recode(term, 
                       "(Intercept)" = "Intercept",
                       anth_risk = "Anthropogenic disturbance", 
                       woody_dist = "Woody Distance",
                       fence_dist = "Fence Distance", 
                       prirds_dist = "Pri Rds Distance",
                       river_dist = "River Distance",
                       secrds_dist = "Sec Rds Distance",
                       waterpts_dist = "Water Point Distance")) %>% 
  ggplot(aes(factor(frac), 
             y = estimate)) +
  geom_boxplot() + 
  facet_wrap(~ term, scale  ="free") +
  geom_jitter(alpha = 0.2) + 
  labs(x = "Available Points per Use Location", 
       y = "Estimate") +
  theme_light()

# This is a key tool in making a final assessment of the minimum # of available points needed for each use location. We can see little change in the coefficient values once we have 20 points per use location, but we'll use 50 points per 'Use' location to be safe.
```

## Model Fitting
Based on the model fitting simulations, we will generate 50 available locations per use location.  More would be better, but there is seemingly little change between model fits as we increase the number of availability locations beyond this threshold.

Here the intercept decreases as the number of available points increases, but the slope parameter estimates, on average, do not change much once we included at least 20 available points per used point.  We conclude, at least in this particular case, 20 available points per used point is sufficient for interpreting the slope coefficients.  We will follow the same process described above to generate random points, variable extraction, and model fitting.  We then plot the parameter responses.

```{r Model Fitting, message=FALSE, warning=FALSE, echo=TRUE}
# Generating 50 available locations per each used location within animal homerange
nt.rsf.50 <- random_points(hr,
                           n = nrow(nt.trk.rs) * 50,
                           type ="regular",  
                           presence = nt.trk.rs) 

# Extracting the covariates
nt.rsf.50 <- nt.rsf.50 %>% extract_covariates(rsf.stack)

# Fit a "full" model after removing waterpoints because of high collinearity. We also should avoid running fence distance or primary roads with woody distance, because they are highly correlated. As a result, let's fit three "full" models and then compare them.

# Fitting Model 1 - No woody distance
M1.NoWood <- glm(case_ ~ scale(anth_risk) + 
                   scale(fence_dist) + 
                   scale(prirds_dist) + 
                   scale(secrds_dist) + 
                   scale(river_dist), 
                 family = binomial(link="logit"), 
                 data = nt.rsf.50)

# Get Summary & calculate profile confidence intervals to see if coefficients overlap zero
summary(M1.NoWood)
# broom::tidy(M2.Wood) # Could also use the tidy summary, which is a little easier to read
confint(M1.NoWood)

# Fitting Model 2 - No Fencing or Primary Roads
M2.Wood <- glm(case_ ~ scale(anth_risk) + 
                 scale(secrds_dist) + 
                 scale(river_dist) +
                 scale(woody_dist), 
               family = binomial(link="logit"), 
               data = nt.rsf.50)

# Get Summary & calculate profile confidence intervals to see if coefficients overlap zero
summary(M2.Wood)
confint(M2.Wood)

# Compare models with AIC
AIC(M1.NoWood, M2.Wood)
```

## Model Interpretation
The AIC summary provides evidence that the M1.NoWood model is superior to our alternative model (M2.Wood).  We can make some assessments of the effect of each predictor simply by looking at the sign (positive or negative) of each coefficient in the summary table.  But what do these coefficients actually mean?  

Because the coefficients are displayed on the scale of log-odds, they are (or can be) difficult to interpret.  To interpret them as odds ratios (referred to as the "Relative Selection Strength" in the RSF world), we need to exponentiate (`exp()`) the coefficients.  We will plot the log-odds and then calculate the Relative Selection Strength of each predictor variable.    

```{r Model Interpretation, message=FALSE, warning=FALSE, echo=TRUE}
# Plot raw, non-transformed estimates (log-odds)
plot_model(M1.NoWood, transform = NULL) 

# This indicates that we have two predictors (river distance and secondary roads) that have positive effects and three variables (anthropogenic risk, fence distance, and primary roads) that have negative effects.  

# Specifically, we see that when we increase human risk (1 unit increase), we reduce (negative sign) the relative use probability.  This is what we would expect (i.e., more human population leads to less wildebeest).  For fences and primary roads, we also see a negative effect.  Here, a 1 unit increase in the distance to fences or primary roads results in a negative effect on the relative use probability of wildebeest.  That is, relative use probability of wildebeest is higher when closer to these features (a positive association), than further away.  This is an unexpected result.  We should remember, however, that we are only investigating 1 animal at this time.

# For secondary roads and rivers, we see positive effects (i.e., an increase in the distance to roads and rivers leads to an increase in the relative use probability of wildebeest).  That is, relative use probability is higher as we move away from these features (a negative association).  This is also unexpected, at least for secondary roads.

# Create an alternative plot of the raw coeeficients (log-odds)
coef_plot <- sjPlot::plot_model(M1.NoWood,
                   type = "est",
                   colors = c("cadetblue", "tomato"),
           transform = NULL,
           title = "Plot of Standardized Coefficents") 
# coef_plot

# We can keep the above plot, or improve it using ggplot.  Here we'll add a horizontal line, centered on 0, so that we can easily see the positive and negative effects.
coef_plot +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             col = "darkgray") +
  theme_classic() 

# Check out help(plot_model), as there are a number of customizable arguments.  You could, for example, use the "terms" argument to indicate which terms you want to show on the plot and also show the odds ratios instead of the log odds (raw coeffients is the defaul).  The odds ratios, however, are harder to interpret when the coefficients are negative, as we'll see below. 
# coef_plot.example <- sjPlot::plot_model(M1.NoWood,
#                    type = "est",
#                    colors = c("cadetblue", "tomato"),
#                    terms = c("scale(anth_risk)","scale(fence_dist)"), # Will only plot the predictors specified
#            transform = "exp", # Exponentiating to plot the odds ratios
#            title = "Plot of Standardized Coefficents") 
# 
# coef_plot.example

# Now let's convert the log odds to odds ratios to evaluate Relative Selection Strength.  We do this by exponentiating the coefficients
my_coefs <- coef(M1.NoWood)
exp(my_coefs)

# Remember: Don't interpret the intercept in RSF models.  This is meaningless.
# Any value below 1 has a negative impact on use probability; Any value above 1 is a positive impact. These values can be interpreted as the relative change in the probability of use with 1 unit of change in the predictor.  Easy to interpret for predictors with a positive sign.

# For negative coefficients, it's easier to include a negative sign when exponentiating
exp(-my_coefs[2:4])

# Thus, for example, we can say a location with a 1 unit increase in anth_risk is 1.20 times LESS likely to be used by this wildebeest.  We see the same general patterns for fence distance (1.72 times LESS likely) and primary road distance (2.55 times LESS likely).

# One last IMPORTANT note: We scaled our predictor variables.  As a result, a 1 unit change is actually a change in 1 STANDARD DEVIATION.  If we did not scale our predictor variables, a 1 unit change in the distance variables would be 1 meter, for example. Just something to be aware of. 
```

## Visualizing Parameter Responses
We can also visualize the responses of each predictor variables by holding all other predictor variables constant.  The `visreg()` function can be used to do this process for us, making plotting easy.  We need to note, however, that the intercept is included when using this function.  The [amt](https://cran.r-project.org/web/packages/amt/index.html) also has a function, `logRSS()`, to calculate Relative Selection Strength across a sequence of values.  Using this function gives us flexibility and is more appropriate to visualize the response.  

```{r Visualize, message=FALSE, warning=FALSE, echo=TRUE}
# Use visreg to visualize
# visreg(M1.NoWood,
#        xvar = "anth_risk",
#        scale="response", 
#        ylab="Relative Use Intensity",
#        xlab="Anthropogenic risk",
#        partial=F,
#        rug=F,
#        line=list(col="black"), 
#        fill=list(col="light gray"))
# 
# # Let's loop over all the variables and plot them
# # Step 1: Create an object holding the variables
# var1 <- c("anth_risk",
#           "fence_dist",
#           "prirds_dist",
#           "secrds_dist",
#           "river_dist")
# 
# # Step 2: Create an object of the variable names (for plotting labels)
# var.names <- c("Anthropogenic Risk Index",
#                "Fence Distance (m)",
#                "Primary Road Distance (m)",
#                "Secondary Roads Distance (m)",
#                "River Distance (m)")
# 
# # Step 3: Set plotting window
# par(mfrow=c(3,2))
# 
# # Step 4: Loop over each variable and plot
# for (i in 1:length(var1)){
#   visreg(M1.NoWood,
#          xvar = var1[i],
#          scale="response", 
#          ylab="Relative Use Intensity",
#          xlab=var.names[i],
#          partial=F,
#          rug=F,
#          line=list(col="black"), 
#          fill=list(col="light gray"))
# }
# 
# # Step 5: Reset plotting window
# par(mfrow=c(1,1))

# Use log_rss()
# Step 1: Create a dataframe of a sequency of values to predict (seq(min to max)).  Note that I am holding all other variables constant (mean values)
df1 <- data.frame(anth_risk = seq(min(nt.rsf.50$anth_risk),
                                    max(nt.rsf.50$anth_risk),
                                    length.out = 100),
                    fence_dist = mean(nt.rsf.50$fence_dist),
                    river_dist = mean(nt.rsf.50$river_dist),
                    prirds_dist = mean(nt.rsf.50$prirds_dist),
                    secrds_dist = mean(nt.rsf.50$secrds_dist))

# Step 2: Create a dataframe of what are comparing to.  Here, I'm comparing to the mean anth_risk value
df2 <- data.frame(anth_risk = mean(nt.rsf.50$anth_risk),
                    fence_dist = mean(nt.rsf.50$fence_dist),
                    river_dist = mean(nt.rsf.50$river_dist),
                    prirds_dist = mean(nt.rsf.50$prirds_dist),
                    secrds_dist = mean(nt.rsf.50$secrds_dist))

# Step 3: Use the log_rss() function to predict across our sequence using coefficients from our model
logRSS_riskRange <- log_rss(object = M1.NoWood,
                            x1 = df1,
                            x2 = df2)

# Look at result
# logRSS_riskRange

# This output is set up nicely for plotting, since we have our input values of risk, and we have our output values for log-RSS. I'd like to plot the odds-ratio values though (as opposed to the log-odds) so in plotting, we'll take the exp() of those values.

# Step 4: Plot
ggplot(logRSS_riskRange$df, 
       aes(x = anth_risk_x1, 
           y = exp(log_rss))) +
  geom_line(linewidth = 1) +
  geom_hline(yintercept = exp(0), 
             linetype = "dashed", 
             color = "gray30") +
  xlab("Anthropogenic Risk") +
  ylab("RSS vs Mean Risk") +
  theme_bw()
  
  # The RSS of 1.0 here crosses the line at the mean value of risk, since that is what this relative value (odds ratio) is being compared to. 
```

## Generating Model Prediction
The last step is to generate a predictive surface of habitat suitability.  We can't use `Predict()` in this case, because we want to ignore the intercept, but we can do all the same things with some extra work.  Also remember that we **must** scale the raster values when making the prediction, since our coefficient estimates are based on scaled values.

```{r Prediction}
# Extract the coefficient of each predictor from the model summary
coeff <- M1.NoWood$coefficients

# Then, use the logistic equation to generate predictions (no B_0)
# w*(x)=exp(β1x1 + β2x2 +.... + βnxn)/exp(1 + β1x1 + β2x2 +.... + βnxn)

# where w*(x) is the relative probability of selection, dependent upon covariates X1 through Xn, and their estimated regression coefficients β1 to βn, respectively.

# IMPORTANT: Since we scaled our parameters, we MUST also scale our raster layers.  The scaled raster layers will look the same, but the range of values being plotted will change.  Here, I'm scaling by subtracting the mean value of the parameter and dividing by the standard deviation.  This is what we did above by using the scale() function above.  See help(scale).

# See how this is written: ScaledRasterLayer = (unscaled raster layer - mean(dataset$predictor)) / sd(dataset$predictor)
anth.scale <- (rsf.stack$anth_risk - mean(nt.rsf.50$anth_risk)) / sd(nt.rsf.50$anth_risk)
fence.scale <- (rsf.stack$fence_dist - mean(nt.rsf.50$fence_dist)) / sd(nt.rsf.50$fence_dist)
prirds.scale <- (rsf.stack$prirds_dist - mean(nt.rsf.50$prirds_dist)) / sd(nt.rsf.50$prirds_dist)
secrds.scale <- (rsf.stack$secrds_dist - mean(nt.rsf.50$secrds_dist)) / sd(nt.rsf.50$secrds_dist)
river.scale <- (rsf.stack$river_dist - mean(nt.rsf.50$river_dist)) / sd(nt.rsf.50$river_dist)

# Prediction - Need to get the coefficient numbers [[i]] correct
pred <- exp(anth.scale*coeff[[2]] + 
              fence.scale*coeff[[3]] + 
              prirds.scale*coeff[[4]] + 
              secrds.scale*coeff[[5]] + 
              river.scale*coeff[[6]]) /
  (1+exp(anth.scale*coeff[[2]] + 
           fence.scale*coeff[[3]] + 
           prirds.scale*coeff[[4]] + 
           secrds.scale*coeff[[5]] + 
           river.scale*coeff[[6]]))

# Provide Spatial Prediction - Based off of the coefficients from this single animal
plot(pred)

# Let's mask the data with the Athi-Kaputiei Boundary file and then use tmap to plot
# We loaded a spatial polygon data layer at the start of this script named "Athi.Bound"
pred.nt <- mask(pred, Athi.Bound)

# Graph on tmap
tm_shape(pred.nt,
         name = "Habitat Suitability") +
  tm_raster(palette = "PuBuGn", n = 10,
            alpha = 0.6,
            title = "Habitat Suitability") +
  tm_shape(Athi.Bound,
           name = "Athi-Kaputiei Plains") +
  tm_polygons(alpha = 0, # Make polygon transparent
              border.col = "green") + 
  tm_layout("Ntishya Example")

# Save this file in your data directory
writeRaster(pred.nt, filename ="./Output/Prediction.nt.tif")
```

## Next Steps

# Fitting a Model for Multiple Animals
More than likely, you will have multiple tagged animals and will be interested in investigating resource use across your entrepopulation (or interested in differences in selection between different populations). This can be accomplished in multiple ways:

* Two-stage approach
  + Fit a glm model for each individuals
  + Average the coefficients
* Account for individual random effects
  + Generalized Linear Mixed Effects Regression (GLMER)
  
## Two Stage Approach
The two stage approach follows the same procedure as described above, except that you would select additional animals and fit a GLM to each individual.  You could then output the summary coefficients and either apply the mean of your coefficients (e.g., `mn.coeff.anth = mean(c(model1.coeff.anth, model2.coeff.anth, model3.coeff.anth))`) to the scaled raster layers or create individual predictions and calculate the mean of the predictive surfaces. Estimating the variance between animals, however, is very difficult.  Model structures must be exactly the same between individuals in order to make comparisons.

Example of the command to calculate the mean of predictions from multiple animals:

`mean.pred <- mean(pred.mask.Animal1,pred.mask.Animal2,pred.mask.Animal3)`

## Generalized Linear Mixed Effects Regression
Mixed effects models are commonly used to provide estimates of the population.  The same process as described above is used, but the model structure is slightly more complicated.  [Amt]() provides a nice structure for creating random points, with a simple process for annotating points with underlying environmental layers.  This information is being provided to those that have animal tracking datasets and plan to work on analyses.

## Generating Use/Availability
The dataframe needs to be converted to a list, with a dataframe for each animal.  The `lapply` command can then be used to make a track (`mk_track`) and generate a set of random points (`random_points`).

```{r GLMER, eval=F, echo=T}
# Create object of ids
wild.An <- unique(wild.Athi$id)
# Creating list that contain the dataframe for each individual
wild.Athi<-split(wild.Athi,wild.Athi$id)

# Creating track for each individual animal in the list
wild.track <- lapply(wild.Athi, function(x) mk_track(x,
         .x=easting,.y=northing,
         .t=timestamp,crs = sp::CRS(AEA.Africa.proj)))

# Let's reduce the file, selecting only 20% of each use dataset
wild.track <- lapply(wild.track, function(x) x[sample(1:nrow(x),size = ceiling(nrow(x)*0.20)),c("x_","y_","t_")])

# Generate 50 random points for each animal 
# Using a list apply here to apply the function
wb.Athi <- lapply(wild.track, function(x) random_points(x, n=nrow(x) * 50,typ="random"))
```

## Extract Environmental Variables
We can then extract the relevant base layer information at each data point, similar to above, but by applying the `extract_covariates` to the list.

```{r GLMER Extract, eval=F, echo=T}
# Adding anthropogenic risk covariate to each dataframe by using extract_covariates function
wb.Athi <- lapply(wb.Athi, function(x) extract_covariates(x, rsf.stack))

# Adding animal ID to each dataframe in the list.  We'll use this as the random effect.
wb.Athi <- mapply(cbind, wb.Athi, "Animal_ID" = wild.An, SIMPLIFY=F) 

# Binding all dataframes in the list into a single dataframe for analysis
wb.Athi <- do.call(rbind,wb.Athi)
```

## Model Fitting
The model structure looks similar to what we have already done together, except we are now adding a random effect (1|Animal)

```{r GLMER Fit, eval=F, echo=T}
# This model will be slow to execute
mixed.model <- glmer(case_ ~ scale(anth_risk) + scale(fence_dist) + scale(waterpts_dist) + scale(prirds_dist) + scale(river_dist) + scale(secrds_dist) + scale(woody_dist) + (1|Animal_ID), 
                family = binomial(link="logit"), 
                data = wb.Athi)

# Print Model Summary
summary(mixed.model)
```

## Create Parameter Responses
Similar to earlier exercises, graph and interpret results

```{r GLMER Response, eval=F, echo=T}
# Plot coefficients and response curves for inference at the population level
plot_model(mixed.model,transform = NULL) 

# Exponentiate the coefficients and plot the odds ratios
tab_model(mixed.model)
plot_model(mixed.model) 

# Graph response curves.  Use effect plot from jtools.  Executes much quicker
effect_plot(mixed.model,data=wb.Athi, 
            pred=anth_risk,interval = TRUE,
            x.label = "Anthropogenic risk",
            y.label = "Relative Selection Strength") + 
  theme_classic()
```

## Generate Model Prediction
Recalculate the scaled rasters and predict a surface

```{r GlMER Prediction, eval=F, echo=T}
# Extract the coefficient of each predictor from the model summary
coef <- summary(mixed.model)
coeff <- coef$coefficients

# Scale rasters:
anth.scale <- (anth_risk - mean(wb.Athi$anth_risk)) / sd(wb.Athi$anth_risk)
fence.scale <- (fence_dist - mean(wb.Athi$fence_dist)) / sd(wb.Athi$fence_dist)
water.scale <- (prirds_dist - mean(wb.Athi$prirds_dist)) / sd(wb.Athi$prirds_dist)
prirds.scale <- (secrds_dist - mean(wb.Athi$secrds_dist)) / sd(wb.Athi$secrds_dist)
river.scale <- (river_dist - mean(wb.Athi$river_dist)) / sd(wb.Athi$river_dist)
secrds.scale <- (waterpts_dist - mean(wb.Athi$waterpts_dist)) / sd(wb.Athi$waterpts_dist) 
woody.scale <- (woody_dist - mean(wb.Athi$woody_dist)) / sd(wb.Athi$woody_dist) 

# Prediction
pred.mixed <- exp(anth.scale*coeff[[2]]+ fence.scale*coeff[[3]] + water.scale*coeff[[4]] + prirds.scale*coeff[[5]] + river.scale*coeff[[6]] + secrds.scale*coeff[[7]] + woody.scale*coeff[[8]])/(1+exp(anth.scale*coeff[[2]]+ fence.scale*coeff[[3]] + water.scale*coeff[[4]] + prirds.scale*coeff[[5]] + river.scale*coeff[[6]] + secrds.scale*coeff[[7]] + woody.scale*coeff[[8]]))

# Provide Spatial Prediction - Based off of the coefficients from this single animal
plot(pred.mixed)

# We could then mask this data layer, and then use tmap an place the layer on top of a map service (like we did with the Addax)
# We loaded a spatial polygon data layer at the start of this script named "Athi_Bound"
# Create new layer and crop it to the analysis extent
pred.mixed.mask <- mask(pred.mixed, Athi_Bound)
pred.mixed.mask <- crop(pred.mixed.mask,y=extent(Athi_Bound))
plot(pred.mixed.mask)
```

# Next Steps/Key Questions
* How to incorporate the animal's movement into the size of the buffer (SSF analysis)?
* How to create multiple competing hypotheses and select the best model based on AIC?
* Does it make sense to assume simple linear effects on distance measures?
